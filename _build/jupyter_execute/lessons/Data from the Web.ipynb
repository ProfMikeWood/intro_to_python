{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86545698-b68a-45c8-ad91-5ff49e2ecc11",
   "metadata": {},
   "source": [
    "# Data from the Web\n",
    "In this lesson, we will explore how to obtain data from the internet using Python.\n",
    "\n",
    "By the end of this lesson, you should be able to:\n",
    "1. Read data from a simple text web page\n",
    "2. Parse data from a web page with html formatting\n",
    "3. Download images from a web page using the image url\n",
    "4. Download data from a website using the download link's url\n",
    "\n",
    "## A Note on Web Scraping\n",
    "Web scraping refers to the gathering of data from a website and storing it on another computer. In the U.S., it is perfectly legal to write programs to gather data from the web as long as you don't use information to harm the company and/or it's website. There are a few guidelines to follow regarding web scraping:\n",
    "1. Don't overwhelm sites by making excessive requests\n",
    "2. Do give attribution to the sites where you retrieve your data\n",
    "3. Only gather data from sites that are publically available.\n",
    "\n",
    "In general, if you are gathering data from public websites for educational purposes (rather than writing code to support a business interest), then you are in the clear. In fact, many websites provide a means to access data from their site in the form of an Application Programmng Interface (API). Many online services such as Google, OpenAI, etc. provide a means by which to access data on their sites with pre-defined tools. We will explore these tools in Lecture 10-2.\n",
    "\n",
    "## Examples for this lesson: National Data Buoy Center\n",
    "To get familiar with web scraping, we are going to download data from various components of NOAA's [National Data Buoy Center website](https://www.ndbc.noaa.gov/). This website hosts oceanographic and meteorological data collected by NOAA (and paid for by U.S. tax payers) is freely available for the public to access and use.\n",
    "\n",
    "#### Import the modules required for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae39624a-16f4-4d14-a14c-1ecf9967edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the requests and BeautifulSoup modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd85e5a-4eae-4ab6-9d65-d22a253d2eeb",
   "metadata": {},
   "source": [
    "### Part 1: Reading simple text data\n",
    "To explore handling data from the web, first we will start with simple text data. One easy set of data to visualize is the data from the National Buoy Data Center. Take a look at an example web page by following the link: https://www.ndbc.noaa.gov/data/realtime2/46092.txt\n",
    "\n",
    "Now, we will access this data using Python using the requests module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6c3171-86d3-4952-8851-1aee6d1fc42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a url to the web page\n",
    "\n",
    "\n",
    "# use the requests package to read the page to a response\n",
    "\n",
    "\n",
    "# print the response status code\n",
    "\n",
    "\n",
    "# print the response status description (reason)\n",
    "\n",
    "\n",
    "# save the page text into a string\n",
    "\n",
    "\n",
    "# split the text by next lines and print the first 3 lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44bf1a1-cc0d-4ea0-ae6f-20add91bd6d5",
   "metadata": {},
   "source": [
    "#### &#x1F914; Mini-Exercise\n",
    "Goal: The National Weather Service produces regional text based forecasts for weather that can be transmitted to boats operating in US Coastal Waters. Read in data from the most recent forecast to find out about current weather alerts. The alerts for San Francisoc Bay area can be accessed at https://tgftp.nws.noaa.gov/data/raw/fz/fzus56.kmtr.cwf.mtr.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e823fd82-4d74-44dd-b6c2-0e8752475f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82957cd8-8a7b-40d9-861b-9c1373140f3a",
   "metadata": {},
   "source": [
    "### Part 2: Parsing data from html-formatted pages\n",
    "Typically, web pages are not just text - they are formatted with HyperText Markup Language (HTML) formatting. Just like regular pages, we can read in html-formatted pages a typical ascii-style text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1d001c-24e5-465f-8287-0faf8fa4b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the url to the station\n",
    "\n",
    "\n",
    "# use the response module to get the data from the url\n",
    "\n",
    "\n",
    "# read in the page text \n",
    "\n",
    "\n",
    "# split the text by lines\n",
    "\n",
    "\n",
    "# search the lines for the one that has the \"Water depth\" information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41571cd3-02e3-4121-a795-9e43b1b6d386",
   "metadata": {},
   "source": [
    "As you can see above, it can be a little cumbersome to search through all of the html code to find what you might be looking for on a website. To obtain data from these sites in a usable format, it is helpful to leverage tools that can help to parse html code. Since html is a common language for web pages, there are several packages to organize and search html-scripted pages. One commonly-used function is Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01aee01c-caa1-4ed5-b67a-d4250ed8b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BeautifulSoup to parse the html data\n",
    "\n",
    "\n",
    "# search for the division with the id \"stn_metadata\"\n",
    "# store it as a variable called stn_metadata\n",
    "\n",
    "\n",
    "# convert the stn_metadata to a string\n",
    "\n",
    "\n",
    "# split the stn_metadata\n",
    "\n",
    "\n",
    "# search for the \"Water depth\" in the stn_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61a33a-2cc0-4707-8b3b-b04a4c8405aa",
   "metadata": {},
   "source": [
    "#### &#x1F914; Mini-Exercise\n",
    "Goal: Find the link to the NDBC's Facebook site.\n",
    "\n",
    "The front page of the NDBC site contains three links to the Facebook, LinkedIn, and Twitter (X) pages for the NDBC social media. These are contained in an html division with the class \"socialMediaContainer\". Use the requests library to open the page and the BeautifulSoup module to parse the html text. Then, find the division with the social media information to find where the link to the Facebook page leads. In particular, print the line with the \"NDBC on Facebook\" string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "698d8752-5e79-42ca-8279-7500776cdce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a18e087-9f16-49ac-b2d0-beda2eb0cf82",
   "metadata": {},
   "source": [
    "### Part 3: Obtaining images from web pages\n",
    "Anything that exists on a web page can be obtained and stored on your local system. For example, images that are hosted on web pages can be stored on your system. Consider again the Monterey Buoy 46092 as described here: https://www.ndbc.noaa.gov/station_page.php?station=46092\n",
    "\n",
    "This page contains an image file for the buoy. Let's find the link to the buoy and download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5547bf66-7af5-47aa-a584-d33e99c58c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide a path to the buoy image\n",
    "\n",
    "\n",
    "# use the requests module to get the image\n",
    "\n",
    "\n",
    "# define an output name for the image\n",
    "\n",
    "\n",
    "# open the file as a writable binary\n",
    "\n",
    "    # iterate through the chunks and write to the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb20fa8-61fa-4aa6-a887-4e4b76e692bf",
   "metadata": {},
   "source": [
    "#### &#x1F914; Mini-Exercise\n",
    "Goal: Find and store an image from your favorite web site. \n",
    "\n",
    "Any publically-available image on the web can be downloaded with the requests module. Download your favorite image in the block below and store it to your system. If you don't have a favorite site or image, you can download this comic: https://imgs.xkcd.com/comics/git_2x.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e13f09b-8967-456d-b70f-d25f1b7963aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lecture_10-1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpimg\u001b[39;00m\n\u001b[1;32m     11\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[0;32m---> 12\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mmpimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img)\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs122/lib/python3.10/site-packages/matplotlib/image.py:1525\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parse\u001b[38;5;241m.\u001b[39murlparse(fname)\u001b[38;5;241m.\u001b[39mscheme) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;66;03m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease open the URL for reading and pass the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult to Pillow, e.g. with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m         )\n\u001b[0;32m-> 1525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimg_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[1;32m   1527\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mPngImagePlugin\u001b[38;5;241m.\u001b[39mPngImageFile) \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m             pil_to_array(image))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/cs122/lib/python3.10/site-packages/PIL/Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3215\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3218\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3219\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lecture_10-1.jpg'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the output_file path\n",
    "output_file = 'lecture_10-1.jpg'\n",
    "\n",
    "# enter your code here\n",
    "\n",
    "\n",
    "\n",
    "# show the image using the code from Homework 4:\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "fig = plt.figure()\n",
    "img = mpimg.imread(output_file)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4906353-c4ec-4700-9299-9314779ee4a4",
   "metadata": {},
   "source": [
    "### Part 4: Downloading data from links\n",
    "A lot of data is provided online with through accessible links to data from remote data servers. For example, the historical buoy data on the NDBC site is stored in compressed format. We can have a look at the available historical data for the Monterey Buoy here: https://www.ndbc.noaa.gov/station_history.php?station=46092\n",
    "\n",
    "Below, let's download the compressed 2022 data stored in the `46092h2022.txt.gz` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f24fa-e831-4e5d-8780-9d6fdad56e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the url here\n",
    "\n",
    "\n",
    "# use the requests module to get the data\n",
    "\n",
    "\n",
    "# define an output file \n",
    "\n",
    "\n",
    "# read the file in as chunks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs122",
   "language": "python",
   "name": "cs122"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}